{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Keras_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "U4-S2-NNF (Python 3.7)",
      "language": "python",
      "name": "u4-s2-nnf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HJzTIkYAsLxw"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Neural Network Frameworks (Keras)\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Lesson 3*\n",
        "\n",
        "# Lets Use Libraries!\n",
        "\n",
        "The objective of the last two days has been to familiarize you with the fundamentals of neural networks: terminology, structure of networks, forward propagation, error/cost functions, backpropagation, epochs, and gradient descent. We have tried to reinforce these topics by requiring to you code some of the simplest neural networks by hand including Perceptrons (single node neural networks) and Multi-Layer Perceptrons also known as Feed-Forward Neural Networks. Continuing to do things by hand would not be the best use of our limited time. You're ready to graduate from doing things by hand and start using some powerful libraries to build cutting-edge predictive models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MFYCXc5KdkPE"
      },
      "source": [
        "# Keras\n",
        "\n",
        "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
        "\n",
        "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "Runs seamlessly on CPU and GPU.\" \n",
        "\n",
        "## Installation\n",
        "\n",
        "The Keras API is particularly straightforward and it already comes pre-installed on Google Colab! \n",
        "\n",
        "<img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/pip-freeze-keras.png\" width=\"300\">\n",
        "\n",
        "If you're not on Google Colab you'll need to install one of the \"backend\" engines that Keras runs on top of. I recommend Tensorflow:\n",
        "\n",
        "> `pip install tensorflow`\n",
        "\n",
        "Google Colab does not have the latest Tensorflow 2.0 installation, so you'll need to upgrade to that if you want to experiment with it. However Tensorflow 2.0 was just released last month and is still in \"alpha\" so if you **really** want to use the latest and greatest be prepared for odd bugs that you don't have control over every once in a while. <https://www.tensorflow.org/install/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QEncs0SOsFMT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "8e04041c-76ad-41a9-b62c-d0277a14a993"
      },
      "source": [
        "# Use pip freeze to see what packages/libraries your notebook has access to\n",
        "# !pip freeze\n",
        "\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxgUUpIKn54a"
      },
      "source": [
        "## Our First Keras Model - Perceptron, Batch epochs\n",
        "\n",
        "1) Load Data\n",
        "\n",
        "2) Define Model\n",
        "\n",
        "3) Compile Model\n",
        "\n",
        "4) Fit Model\n",
        "\n",
        "5) Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Md5D67XwqVAf"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "Our life is going to be easier if our data is already cleaned up and numeric, so lets use this dataset from Jason Brownlee that is already numeric and has no column headers so we'll need to slice off the last column of data to act as our y values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bn09phMBpY1J",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "dataset = pd.read_csv(url, header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xpZb7f4NX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "903f2685-6a9b-44eb-f7d2-580008e55f69"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1   2   3    4     5      6   7  8\n",
              "0  6  148  72  35    0  33.6  0.627  50  1\n",
              "1  1   85  66  29    0  26.6  0.351  31  0\n",
              "2  8  183  64   0    0  23.3  0.672  32  1\n",
              "3  1   89  66  23   94  28.1  0.167  21  0\n",
              "4  0  137  40  35  168  43.1  2.288  33  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSuitUb4ETQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f957ac3-48e1-43c0-b13a-ba870564843a"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FKuofD3Pogil",
        "outputId": "3919b1d2-96d0-42ee-9c95-164cce912686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "X = dataset.values[:,0:8]\n",
        "print(X.shape)\n",
        "print(X)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0DL2eH_4NYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "0ad2b6fe-100f-4a6e-e3a7-25c1b2594ce2"
      },
      "source": [
        "y = dataset.values[:,-1]\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768,)\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0xMqOyTs5xt"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bp9USczrfu6M",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(812)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1NbE4xZ4sOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequential is a way to stack layers and get an output out of it\n",
        "# help(Sequential)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SoJJi274--j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense is an actual layer type\n",
        "# Main parameter how many\n",
        "# help(Dense)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wAzHLg27thoN"
      },
      "source": [
        "I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DSNsL49Xp6KI",
        "colab": {}
      },
      "source": [
        "# https://keras.io/getting-started/sequential-model-guide/\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZCYX6QzJtvpG"
      },
      "source": [
        "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
        "\n",
        " > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n",
        " \n",
        " The first argument is how many neurons we want to have in that layer. To create a perceptron model we will just set it to 1. We will tell it that there will be 8 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNzOLidxtvFa",
        "outputId": "b196229c-4d8d-4e72-91df-b355b3ec3760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# What are we saying?\n",
        "# 1 neuron, input dimension 8 (8 columns of data)\n",
        "# 'sigmoid' - becuase output is simple 0, 1\n",
        "model.add(Dense(1,input_dim=8,activation='sigmoid'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EnI3jwKMtBL2"
      },
      "source": [
        "### Compile Model\n",
        "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qp6xwYaqurRO",
        "colab": {}
      },
      "source": [
        "# LOSS - how we calculate the loss - this is the error function.\n",
        "#        if multiple catagories - catigorical_crossentropy\n",
        "# OPTIMIZER - makes out model better by using a optimization algo.\n",
        "# METRICS - imforms what we are actually calculating.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDRzXgiu4NY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc255a72-6ed7-491b-e8ee-84591049573e"
      },
      "source": [
        "# pass through data and specify epochs or pass throughs\n",
        "model.fit(X,y, epochs = 150)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 768 samples\n",
            "Epoch 1/150\n",
            "768/768 [==============================] - 0s 373us/sample - loss: 49.3866 - acc: 0.6510\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 46.0513 - acc: 0.6510\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 42.7514 - acc: 0.6510\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 39.3999 - acc: 0.6510\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 36.1108 - acc: 0.6510\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 32.7845 - acc: 0.6510\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 29.5067 - acc: 0.6510\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 26.1673 - acc: 0.6510\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 22.8546 - acc: 0.6510\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 19.6282 - acc: 0.6523\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 16.7176 - acc: 0.6510\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 14.2003 - acc: 0.6536\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 12.0755 - acc: 0.6471\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 10.3146 - acc: 0.6549\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 9.0467 - acc: 0.6497\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 8.0941 - acc: 0.6354\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 7.3022 - acc: 0.6367\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 6.5822 - acc: 0.6445\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 5.9104 - acc: 0.6250\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 5.2388 - acc: 0.6185\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 4.5820 - acc: 0.6198\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 3.9505 - acc: 0.6224\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 3.3448 - acc: 0.6224\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 2.7847 - acc: 0.6198\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 2.3387 - acc: 0.6224\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 1.9625 - acc: 0.6185\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 1.7225 - acc: 0.6120\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 1.5773 - acc: 0.6016\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 1.4815 - acc: 0.5990\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 1.4135 - acc: 0.5951\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 1.3402 - acc: 0.6003\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 1.2736 - acc: 0.6003\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 1.2175 - acc: 0.5951\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 1.1684 - acc: 0.6055\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 1.1274 - acc: 0.6003\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 1.0809 - acc: 0.6133\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 1.0440 - acc: 0.6081\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 1.0083 - acc: 0.6237\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.9876 - acc: 0.6263\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.9606 - acc: 0.6328\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.9301 - acc: 0.6302\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.9083 - acc: 0.6406\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.8909 - acc: 0.6458\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.8836 - acc: 0.6680\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.8639 - acc: 0.6523\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.8448 - acc: 0.6576\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.8322 - acc: 0.6667\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.8224 - acc: 0.6510\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.8153 - acc: 0.6693\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.7981 - acc: 0.6641\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.7900 - acc: 0.6784\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.7809 - acc: 0.6732\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.7754 - acc: 0.6667\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.7625 - acc: 0.6719\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.7562 - acc: 0.6706\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.7496 - acc: 0.6797\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.7491 - acc: 0.6719\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.7396 - acc: 0.6784\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.7329 - acc: 0.6706\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.7189 - acc: 0.6771\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.7142 - acc: 0.6836\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.7122 - acc: 0.6732\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.7091 - acc: 0.6836\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.7016 - acc: 0.6784\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6962 - acc: 0.6849\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6970 - acc: 0.6888\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6903 - acc: 0.6823\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6849 - acc: 0.6810\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6833 - acc: 0.6758\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6793 - acc: 0.6641\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6690 - acc: 0.6862\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6720 - acc: 0.6888\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6651 - acc: 0.6862\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6568 - acc: 0.6823\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6632 - acc: 0.6615\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.6605 - acc: 0.6797\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6568 - acc: 0.6797\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6505 - acc: 0.6823\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6471 - acc: 0.6797\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6522 - acc: 0.6849\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6536 - acc: 0.6680\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6577 - acc: 0.6875\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6463 - acc: 0.6784\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6389 - acc: 0.6784\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6315 - acc: 0.6862\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6368 - acc: 0.6940\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6404 - acc: 0.6745\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6276 - acc: 0.6940\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6345 - acc: 0.6836\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6363 - acc: 0.6654\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6311 - acc: 0.6784\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6271 - acc: 0.6745\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6273 - acc: 0.6823\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6252 - acc: 0.6901\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6216 - acc: 0.6888\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6184 - acc: 0.6849\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6167 - acc: 0.6953\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6218 - acc: 0.6667\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6214 - acc: 0.6797\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6194 - acc: 0.6849\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6163 - acc: 0.6914\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6142 - acc: 0.6966\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6165 - acc: 0.6901\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 0.6172 - acc: 0.6836\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6143 - acc: 0.6836\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6141 - acc: 0.6849\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6116 - acc: 0.6953\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6120 - acc: 0.6914\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6130 - acc: 0.6901\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6157 - acc: 0.6953\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6160 - acc: 0.6849\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6104 - acc: 0.7005\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6143 - acc: 0.6888\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6174 - acc: 0.6849\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6093 - acc: 0.7044\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6086 - acc: 0.6836\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6088 - acc: 0.6979\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6072 - acc: 0.6966\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6171 - acc: 0.6823\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6096 - acc: 0.6927\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6091 - acc: 0.6940\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6135 - acc: 0.6914\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6177 - acc: 0.6797\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6142 - acc: 0.6784\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6030 - acc: 0.7031\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6208 - acc: 0.6680\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6047 - acc: 0.7044\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6043 - acc: 0.6927\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6017 - acc: 0.7070\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6073 - acc: 0.6966\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6297 - acc: 0.6732\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6191 - acc: 0.6888\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6182 - acc: 0.6797\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6054 - acc: 0.6992\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6097 - acc: 0.6862\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6063 - acc: 0.7057\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6014 - acc: 0.7031\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 33us/sample - loss: 0.6119 - acc: 0.6979\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6070 - acc: 0.6927\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.6125 - acc: 0.6758\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.5993 - acc: 0.6901\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6000 - acc: 0.6953\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6006 - acc: 0.6979\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.5976 - acc: 0.7083\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6004 - acc: 0.6849\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 34us/sample - loss: 0.6118 - acc: 0.6914\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6014 - acc: 0.7031\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 35us/sample - loss: 0.5992 - acc: 0.7070\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.5985 - acc: 0.7070\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6007 - acc: 0.6953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f079445eb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrIrpVk4NY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ad640b3-463b-4dec-813e-4419c88106c2"
      },
      "source": [
        "sum(y) / len(y) # Predicting never diabetes is 65%"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3489583333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN8LSTs38TKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "548131fc-032f-4d59-e449-55c3803e12d1"
      },
      "source": [
        "dir(model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_TF_MODULE_IGNORED_PROPERTIES',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_activity_regularizer',\n",
              " '_add_inbound_node',\n",
              " '_add_unique_metric_name',\n",
              " '_add_variable_with_custom_getter',\n",
              " '_assert_compile_was_called',\n",
              " '_assert_weights_created',\n",
              " '_autocast',\n",
              " '_base_init',\n",
              " '_build_input_shape',\n",
              " '_build_model_with_inputs',\n",
              " '_cache_output_metric_attributes',\n",
              " '_call_accepts_kwargs',\n",
              " '_call_arg_was_passed',\n",
              " '_call_fn_args',\n",
              " '_callable_losses',\n",
              " '_check_call_args',\n",
              " '_check_trainable_weights_consistency',\n",
              " '_checkpoint_dependencies',\n",
              " '_clear_losses',\n",
              " '_collect_input_masks',\n",
              " '_collected_trainable_weights',\n",
              " '_compile_distribution',\n",
              " '_compile_eagerly',\n",
              " '_compile_from_inputs',\n",
              " '_compile_metric_functions',\n",
              " '_compile_metrics',\n",
              " '_compile_time_distribution_strategy',\n",
              " '_compile_weighted_metrics',\n",
              " '_compile_weights_loss_and_weighted_metrics',\n",
              " '_compiled_trainable_state',\n",
              " '_compute_dtype',\n",
              " '_compute_output_and_mask_jointly',\n",
              " '_deferred_dependencies',\n",
              " '_distributed_function_cache',\n",
              " '_distributed_model_cache',\n",
              " '_distribution_standardize_user_data',\n",
              " '_distribution_strategy',\n",
              " '_dtype',\n",
              " '_dtype_defaulted_to_floatx',\n",
              " '_dtype_policy',\n",
              " '_dynamic',\n",
              " '_eager_add_metric',\n",
              " '_eager_losses',\n",
              " '_expects_mask_arg',\n",
              " '_expects_training_arg',\n",
              " '_experimental_run_tf_function',\n",
              " '_feed_input_names',\n",
              " '_feed_input_shapes',\n",
              " '_feed_inputs',\n",
              " '_feed_loss_fns',\n",
              " '_feed_output_names',\n",
              " '_feed_output_shapes',\n",
              " '_feed_sample_weights',\n",
              " '_feed_targets',\n",
              " '_flatten',\n",
              " '_function_kwargs',\n",
              " '_gather_children_attribute',\n",
              " '_gather_saveables_for_checkpoint',\n",
              " '_get_call_arg_value',\n",
              " '_get_callback_model',\n",
              " '_get_existing_metric',\n",
              " '_get_node_attribute_at_index',\n",
              " '_get_trainable_state',\n",
              " '_get_training_eval_metrics',\n",
              " '_graph',\n",
              " '_graph_network_add_loss',\n",
              " '_graph_network_add_metric',\n",
              " '_handle_activity_regularization',\n",
              " '_handle_deferred_dependencies',\n",
              " '_handle_metrics',\n",
              " '_handle_per_output_metrics',\n",
              " '_handle_weight_regularization',\n",
              " '_in_multi_worker_mode',\n",
              " '_inbound_nodes',\n",
              " '_init_call_fn_args',\n",
              " '_init_distributed_function_cache_if_not_compiled',\n",
              " '_init_graph_network',\n",
              " '_init_metric_attributes',\n",
              " '_init_set_name',\n",
              " '_init_subclassed_network',\n",
              " '_input_coordinates',\n",
              " '_input_layers',\n",
              " '_insert_layers',\n",
              " '_is_compiled',\n",
              " '_is_graph_network',\n",
              " '_is_layer',\n",
              " '_keras_api_names',\n",
              " '_keras_api_names_v1',\n",
              " '_layer_call_argspecs',\n",
              " '_layers',\n",
              " '_list_extra_dependencies_for_serialization',\n",
              " '_list_functions_for_serialization',\n",
              " '_lookup_dependency',\n",
              " '_loss_weights_list',\n",
              " '_losses',\n",
              " '_make_callback_model',\n",
              " '_make_execution_function',\n",
              " '_make_predict_function',\n",
              " '_make_test_function',\n",
              " '_make_train_function',\n",
              " '_maybe_build',\n",
              " '_maybe_cast_inputs',\n",
              " '_maybe_create_attribute',\n",
              " '_maybe_initialize_trackable',\n",
              " '_maybe_load_initial_epoch_from_ckpt',\n",
              " '_metrics',\n",
              " '_name',\n",
              " '_name_based_attribute_restore',\n",
              " '_name_based_restores',\n",
              " '_name_scope',\n",
              " '_nested_inputs',\n",
              " '_nested_outputs',\n",
              " '_network_nodes',\n",
              " '_no_dependency',\n",
              " '_nodes_by_depth',\n",
              " '_non_trainable_weights',\n",
              " '_obj_reference_counts',\n",
              " '_object_identifier',\n",
              " '_outbound_nodes',\n",
              " '_output_coordinates',\n",
              " '_output_layers',\n",
              " '_output_loss_metrics',\n",
              " '_output_mask_cache',\n",
              " '_output_shape_cache',\n",
              " '_output_tensor_cache',\n",
              " '_per_output_metrics',\n",
              " '_per_output_weighted_metrics',\n",
              " '_preload_simple_restoration',\n",
              " '_prepare_output_masks',\n",
              " '_prepare_sample_weights',\n",
              " '_prepare_skip_target_masks',\n",
              " '_prepare_total_loss',\n",
              " '_prepare_validation_data',\n",
              " '_process_target_tensor_for_compile',\n",
              " '_recompile_weights_loss_and_weighted_metrics',\n",
              " '_restore_from_checkpoint_position',\n",
              " '_reuse',\n",
              " '_run_eagerly',\n",
              " '_run_internal_graph',\n",
              " '_sample_weight_modes',\n",
              " '_scope',\n",
              " '_select_training_loop',\n",
              " '_self_name_based_restores',\n",
              " '_self_setattr_tracking',\n",
              " '_self_unconditional_checkpoint_dependencies',\n",
              " '_self_unconditional_deferred_dependencies',\n",
              " '_self_unconditional_dependency_names',\n",
              " '_self_update_uid',\n",
              " '_set_connectivity_metadata_',\n",
              " '_set_dtype_policy',\n",
              " '_set_input_attrs',\n",
              " '_set_inputs',\n",
              " '_set_mask_metadata',\n",
              " '_set_metric_attributes',\n",
              " '_set_optimizer',\n",
              " '_set_output_attrs',\n",
              " '_set_output_names',\n",
              " '_set_per_output_metric_attributes',\n",
              " '_set_trainable_state',\n",
              " '_setattr_tracking',\n",
              " '_should_compute_mask',\n",
              " '_single_restoration_from_checkpoint_position',\n",
              " '_standardize_user_data',\n",
              " '_symbolic_add_metric',\n",
              " '_symbolic_call',\n",
              " '_targets',\n",
              " '_tf_api_names',\n",
              " '_tf_api_names_v1',\n",
              " '_thread_local',\n",
              " '_track_layers',\n",
              " '_track_trackable',\n",
              " '_trackable_saver',\n",
              " '_tracking_metadata',\n",
              " '_trainable',\n",
              " '_trainable_weights',\n",
              " '_training_endpoints',\n",
              " '_unconditional_checkpoint_dependencies',\n",
              " '_unconditional_dependency_names',\n",
              " '_unique_trainable_weights',\n",
              " '_update_sample_weight_modes',\n",
              " '_update_uid',\n",
              " '_updated_config',\n",
              " '_updates',\n",
              " '_validate_compile_param_for_distribution_strategy',\n",
              " '_validate_graph_inputs_and_outputs',\n",
              " '_validate_or_infer_batch_size',\n",
              " '_warn_about_input_casting',\n",
              " 'activity_regularizer',\n",
              " 'add',\n",
              " 'add_loss',\n",
              " 'add_metric',\n",
              " 'add_update',\n",
              " 'add_variable',\n",
              " 'add_weight',\n",
              " 'apply',\n",
              " 'build',\n",
              " 'built',\n",
              " 'call',\n",
              " 'compile',\n",
              " 'compute_mask',\n",
              " 'compute_output_shape',\n",
              " 'compute_output_signature',\n",
              " 'count_params',\n",
              " 'dtype',\n",
              " 'dynamic',\n",
              " 'evaluate',\n",
              " 'evaluate_generator',\n",
              " 'fit',\n",
              " 'fit_generator',\n",
              " 'from_config',\n",
              " 'get_config',\n",
              " 'get_input_at',\n",
              " 'get_input_mask_at',\n",
              " 'get_input_shape_at',\n",
              " 'get_layer',\n",
              " 'get_losses_for',\n",
              " 'get_output_at',\n",
              " 'get_output_mask_at',\n",
              " 'get_output_shape_at',\n",
              " 'get_updates_for',\n",
              " 'get_weights',\n",
              " 'history',\n",
              " 'inbound_nodes',\n",
              " 'input',\n",
              " 'input_mask',\n",
              " 'input_names',\n",
              " 'input_shape',\n",
              " 'input_spec',\n",
              " 'inputs',\n",
              " 'layers',\n",
              " 'load_weights',\n",
              " 'loss',\n",
              " 'loss_functions',\n",
              " 'loss_weights',\n",
              " 'losses',\n",
              " 'metrics',\n",
              " 'metrics_names',\n",
              " 'name',\n",
              " 'name_scope',\n",
              " 'non_trainable_variables',\n",
              " 'non_trainable_weights',\n",
              " 'optimizer',\n",
              " 'outbound_nodes',\n",
              " 'output',\n",
              " 'output_mask',\n",
              " 'output_names',\n",
              " 'output_shape',\n",
              " 'outputs',\n",
              " 'pop',\n",
              " 'predict',\n",
              " 'predict_classes',\n",
              " 'predict_function',\n",
              " 'predict_generator',\n",
              " 'predict_on_batch',\n",
              " 'predict_proba',\n",
              " 'reset_metrics',\n",
              " 'reset_states',\n",
              " 'run_eagerly',\n",
              " 'sample_weight_mode',\n",
              " 'sample_weights',\n",
              " 'save',\n",
              " 'save_weights',\n",
              " 'set_weights',\n",
              " 'state_updates',\n",
              " 'stateful',\n",
              " 'stop_training',\n",
              " 'submodules',\n",
              " 'summary',\n",
              " 'supports_masking',\n",
              " 'test_function',\n",
              " 'test_on_batch',\n",
              " 'to_json',\n",
              " 'to_yaml',\n",
              " 'total_loss',\n",
              " 'train_function',\n",
              " 'train_on_batch',\n",
              " 'trainable',\n",
              " 'trainable_variables',\n",
              " 'trainable_weights',\n",
              " 'updates',\n",
              " 'variables',\n",
              " 'weights',\n",
              " 'with_name_scope']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR8LFVnA8yVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model with pickle\n",
        "# import pickle\n",
        "\n",
        "# model_dump = pickle.dumps(model)\n",
        "# model_dump\n",
        "\n",
        "# model_reborn = pickle.loads(model_dump)\n",
        "# model_reborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghV_fMrI4NY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "57bc49f3-797d-4874-de4d-d6a738440ec0"
      },
      "source": [
        "scores = model.evaluate(X,y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 42us/sample - loss: 0.5960 - acc: 0.6992\n",
            "acc: 69.921875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeDrYiKJ9zk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "471ce992-0e66-478d-f97c-58934c2cb192"
      },
      "source": [
        "# give scores and accuracy\n",
        "scores"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5959828396638235, 0.69921875]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbUfn7rz4NZE",
        "colab_type": "text"
      },
      "source": [
        "#### Adam Optimizer\n",
        "Check out this links for more background on the Adam optimizer and Stohastic Gradient Descent\n",
        "* [Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "* [Adam Optimizer - original paper](https://arxiv.org/abs/1412.6980)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5dW8SZ2Ls9SX"
      },
      "source": [
        "### Fit Model\n",
        "\n",
        "Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nJxdmX_-u5MJ",
        "colab": {}
      },
      "source": [
        "#model.fit(X,y, epochs=150, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz4KAQsP4NZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "f4ab020e-c698-4534-8b05-46758edf584b"
      },
      "source": [
        "model_imporoved = Sequential()\n",
        "model_imporoved.add(Dense(4, input_dim=8, activation=\"relu\")) # INPUT LAYER\n",
        "model_imporoved.add(Dense(3, activation=\"relu\")) # HIDDEN LAYER\n",
        "model_imporoved.add(Dense(1, input_dim=8, activation=\"sigmoid\")) # OUTPUT LAYER\n",
        "model_imporoved.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_imporoved.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 3)                 15        \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 55\n",
            "Trainable params: 55\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Tl77qIFfYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1c93d2b-70ab-4a6c-8d78-30da8051a3df"
      },
      "source": [
        "model_imporoved.fit(X, y, epochs=150)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 768 samples\n",
            "Epoch 1/150\n",
            "768/768 [==============================] - 0s 137us/sample - loss: 1.0956 - acc: 0.4167\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.9180 - acc: 0.4388\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.8171 - acc: 0.4609\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.7626 - acc: 0.4753\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.7309 - acc: 0.5052\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.7150 - acc: 0.5417\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.7035 - acc: 0.5677\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6942 - acc: 0.5951\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6955 - acc: 0.6302\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6827 - acc: 0.6276\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6779 - acc: 0.6328\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6743 - acc: 0.6445\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6713 - acc: 0.6471\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 0.6701 - acc: 0.6497\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6652 - acc: 0.6497\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6636 - acc: 0.6497\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6609 - acc: 0.6497\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6589 - acc: 0.6497\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6568 - acc: 0.6497\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6554 - acc: 0.6497\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6539 - acc: 0.6497\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6533 - acc: 0.6497\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6516 - acc: 0.6497\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6500 - acc: 0.6497\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6492 - acc: 0.6497\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6480 - acc: 0.6497\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6476 - acc: 0.6497\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6464 - acc: 0.6497\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6453 - acc: 0.6497\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6469 - acc: 0.6510\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6440 - acc: 0.6510\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6442 - acc: 0.6510\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6444 - acc: 0.6510\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6438 - acc: 0.6510\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6430 - acc: 0.6510\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6430 - acc: 0.6510\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 49us/sample - loss: 0.6443 - acc: 0.6510\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6423 - acc: 0.6510\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6423 - acc: 0.6510\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6422 - acc: 0.6510\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6421 - acc: 0.6510\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6412 - acc: 0.6510\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6413 - acc: 0.6510\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6411 - acc: 0.6510\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6401 - acc: 0.6510\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6399 - acc: 0.6510\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6395 - acc: 0.6510\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6380 - acc: 0.6510\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6386 - acc: 0.6510\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6383 - acc: 0.6510\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6374 - acc: 0.6510\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6371 - acc: 0.6510\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6365 - acc: 0.6510\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6371 - acc: 0.6510\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6366 - acc: 0.6510\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6371 - acc: 0.6510\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6369 - acc: 0.6510\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6363 - acc: 0.6510\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6356 - acc: 0.6510\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6372 - acc: 0.6510\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 0.6358 - acc: 0.6510\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 51us/sample - loss: 0.6374 - acc: 0.6510\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6364 - acc: 0.6510\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6363 - acc: 0.6510\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6368 - acc: 0.6510\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6356 - acc: 0.6510\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6355 - acc: 0.6510\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6353 - acc: 0.6510\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6365 - acc: 0.6510\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 56us/sample - loss: 0.6353 - acc: 0.6510\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6349 - acc: 0.6510\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6354 - acc: 0.6510\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6352 - acc: 0.6510\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 53us/sample - loss: 0.6353 - acc: 0.6510\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6351 - acc: 0.6510\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6355 - acc: 0.6510\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6350 - acc: 0.6510\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6351 - acc: 0.6510\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6352 - acc: 0.6510\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.6349 - acc: 0.6510\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6347 - acc: 0.6510\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 48us/sample - loss: 0.6353 - acc: 0.6510\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6352 - acc: 0.6510\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.6345 - acc: 0.6510\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 53us/sample - loss: 0.6344 - acc: 0.6510\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 46us/sample - loss: 0.6348 - acc: 0.6510\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.6341 - acc: 0.6510\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 50us/sample - loss: 0.6344 - acc: 0.6510\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6335 - acc: 0.6510\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6332 - acc: 0.6510\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6334 - acc: 0.6510\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6329 - acc: 0.6510\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6328 - acc: 0.6510\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6327 - acc: 0.6510\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6328 - acc: 0.6510\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6329 - acc: 0.6510\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6320 - acc: 0.6510\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6321 - acc: 0.6510\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6307 - acc: 0.6510\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.6302 - acc: 0.6510\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6303 - acc: 0.6510\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6304 - acc: 0.6510\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 47us/sample - loss: 0.6308 - acc: 0.6510\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6309 - acc: 0.6510\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6299 - acc: 0.6510\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6307 - acc: 0.6510\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6304 - acc: 0.6510\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6301 - acc: 0.6510\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6318 - acc: 0.6510\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6298 - acc: 0.6510\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6305 - acc: 0.6510\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6299 - acc: 0.6510\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6302 - acc: 0.6510\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6298 - acc: 0.6510\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6299 - acc: 0.6510\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6305 - acc: 0.6510\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6309 - acc: 0.6510\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6298 - acc: 0.6510\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6296 - acc: 0.6510\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 48us/sample - loss: 0.6290 - acc: 0.6510\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6296 - acc: 0.6510\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6294 - acc: 0.6510\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6302 - acc: 0.6510\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6291 - acc: 0.6510\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6319 - acc: 0.6510\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6302 - acc: 0.6510\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6296 - acc: 0.6510\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6292 - acc: 0.6510\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6294 - acc: 0.6510\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6301 - acc: 0.6510\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 45us/sample - loss: 0.6293 - acc: 0.6510\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 48us/sample - loss: 0.6290 - acc: 0.6510\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6294 - acc: 0.6510\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6292 - acc: 0.6510\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6297 - acc: 0.6510\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6293 - acc: 0.6510\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6287 - acc: 0.6510\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 36us/sample - loss: 0.6307 - acc: 0.6510\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 42us/sample - loss: 0.6287 - acc: 0.6510\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6290 - acc: 0.6510\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6304 - acc: 0.6510\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 41us/sample - loss: 0.6281 - acc: 0.6510\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6302 - acc: 0.6510\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 39us/sample - loss: 0.6295 - acc: 0.6510\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 44us/sample - loss: 0.6300 - acc: 0.6510\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6284 - acc: 0.6510\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 40us/sample - loss: 0.6296 - acc: 0.6510\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 43us/sample - loss: 0.6292 - acc: 0.6510\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 37us/sample - loss: 0.6300 - acc: 0.6510\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 38us/sample - loss: 0.6289 - acc: 0.6510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07807e0d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSybwWEJtGFm"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "34wh8z9MvFMp",
        "outputId": "4d1af593-0017-4e19-eb2b-c75fb2e3ac1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "scores = model_imporoved.evaluate(X,y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 66us/sample - loss: 0.7232 - acc: 0.6510\n",
            "acc: 65.10416865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIJoRBxHy27n"
      },
      "source": [
        "# Keras Perceptron Model in 4 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQxyONqKvFxB",
        "outputId": "12966e66-2297-4f82-85b3-c275a9c38563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5216
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "768/768 [==============================] - 0s 136us/sample - loss: 22.7294 - acc: 0.6406\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 26us/sample - loss: 21.3631 - acc: 0.6380\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 20.0952 - acc: 0.6315\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 18.8854 - acc: 0.6328\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 17.7174 - acc: 0.6393\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 16.6509 - acc: 0.6419\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 15.6339 - acc: 0.6380\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 26us/sample - loss: 14.6817 - acc: 0.6432\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 13.7930 - acc: 0.6484\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 12.9592 - acc: 0.6445\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 12.1468 - acc: 0.6393\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 26us/sample - loss: 11.3390 - acc: 0.6328\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 10.5535 - acc: 0.6367\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 9.8117 - acc: 0.6367\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 9.0918 - acc: 0.6328\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 8.3945 - acc: 0.6367\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 7.7571 - acc: 0.6393\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 7.1795 - acc: 0.6406\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 6.6509 - acc: 0.6419\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 6.1921 - acc: 0.6419\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 5.7899 - acc: 0.6523\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 26us/sample - loss: 5.4369 - acc: 0.6536\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 5.1051 - acc: 0.6549\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 27us/sample - loss: 4.8381 - acc: 0.6549\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 4.5881 - acc: 0.6536\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 4.3488 - acc: 0.6484\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 4.1443 - acc: 0.6549\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 3.9658 - acc: 0.6497\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 3.7958 - acc: 0.6536\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 3.6394 - acc: 0.6576\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 3.5137 - acc: 0.6602\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 3.3749 - acc: 0.6576\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 3.2576 - acc: 0.6615\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 3.1513 - acc: 0.6641\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 3.0349 - acc: 0.6667\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 2.9325 - acc: 0.6654\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 2.8307 - acc: 0.6589\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 2.7398 - acc: 0.6680\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 2.6446 - acc: 0.6576\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 2.5500 - acc: 0.6732\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 2.4688 - acc: 0.6693\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 2.3852 - acc: 0.6602\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 2.2889 - acc: 0.6732\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 2.2106 - acc: 0.6667\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 2.1257 - acc: 0.6667\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 2.0531 - acc: 0.6706\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.9743 - acc: 0.6719\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 1.9052 - acc: 0.6745\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.8393 - acc: 0.6771\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.7765 - acc: 0.6654\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.7120 - acc: 0.6810\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.6583 - acc: 0.6875\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.6018 - acc: 0.6693\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.5400 - acc: 0.6836\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.5016 - acc: 0.6745\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.4510 - acc: 0.6745\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.3902 - acc: 0.6927\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.3516 - acc: 0.6719\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.3241 - acc: 0.6823\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.2665 - acc: 0.6862\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.2320 - acc: 0.6823\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.1958 - acc: 0.6810\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 1.1606 - acc: 0.6849\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 1.1273 - acc: 0.6836\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.1082 - acc: 0.6953\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.0792 - acc: 0.6797\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.0442 - acc: 0.6901\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 1.0129 - acc: 0.6810\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 1.0032 - acc: 0.6927\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.9585 - acc: 0.6810\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.9485 - acc: 0.6888\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.9205 - acc: 0.6888\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.9012 - acc: 0.6862\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.8811 - acc: 0.6823\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.8624 - acc: 0.6875\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.8433 - acc: 0.6862\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.8375 - acc: 0.6927\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.8264 - acc: 0.6927\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.8026 - acc: 0.6901\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 0.7855 - acc: 0.6901\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.7769 - acc: 0.6849\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.7696 - acc: 0.6966\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.7548 - acc: 0.6901\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.7487 - acc: 0.6875\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.7354 - acc: 0.6940\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.7271 - acc: 0.6992\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.7236 - acc: 0.6927\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 0.7124 - acc: 0.6992\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.7052 - acc: 0.6927\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.7165 - acc: 0.6875\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6901 - acc: 0.6992\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6898 - acc: 0.6966\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6791 - acc: 0.6953\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6776 - acc: 0.6979\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6714 - acc: 0.6940\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6658 - acc: 0.6953\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 0.6601 - acc: 0.6927\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6669 - acc: 0.6888\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6632 - acc: 0.6758\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6534 - acc: 0.7057\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6655 - acc: 0.6771\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6503 - acc: 0.6901\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6438 - acc: 0.6875\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6386 - acc: 0.6927\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 20us/sample - loss: 0.6427 - acc: 0.6810\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6367 - acc: 0.6901\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6322 - acc: 0.6979\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6313 - acc: 0.6966\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 20us/sample - loss: 0.6258 - acc: 0.6888\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6343 - acc: 0.6810\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6195 - acc: 0.6992\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6292 - acc: 0.6940\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6368 - acc: 0.6745\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6279 - acc: 0.6771\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6326 - acc: 0.7005\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6390 - acc: 0.6862\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6140 - acc: 0.7031\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6108 - acc: 0.6940\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6116 - acc: 0.6992\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 20us/sample - loss: 0.6133 - acc: 0.6979\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6272 - acc: 0.6914\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6120 - acc: 0.6979\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6103 - acc: 0.7057\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6139 - acc: 0.6875\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 24us/sample - loss: 0.6078 - acc: 0.6992\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 27us/sample - loss: 0.6145 - acc: 0.6940\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 20us/sample - loss: 0.6083 - acc: 0.6927\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 25us/sample - loss: 0.6048 - acc: 0.6875\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6038 - acc: 0.6914\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6029 - acc: 0.6953\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6033 - acc: 0.6979\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6029 - acc: 0.6992\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6050 - acc: 0.6953\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6086 - acc: 0.7070\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 23us/sample - loss: 0.6017 - acc: 0.6927\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5996 - acc: 0.6940\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.6021 - acc: 0.6927\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5982 - acc: 0.7096\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5965 - acc: 0.6927\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.5976 - acc: 0.6888\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6167 - acc: 0.6940\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 20us/sample - loss: 0.5959 - acc: 0.7031\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.5967 - acc: 0.7057\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5961 - acc: 0.7005\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.6016 - acc: 0.7018\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5960 - acc: 0.6979\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.5937 - acc: 0.7044\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5930 - acc: 0.7005\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 22us/sample - loss: 0.5967 - acc: 0.6992\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 21us/sample - loss: 0.5954 - acc: 0.6927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0xb37702e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z1wfKUxszPKa",
        "outputId": "0cdacd1d-6e5a-4bbe-fabb-568cd94724be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 43us/sample - loss: 0.5944 - acc: 0.6836\n",
            "acc: 68.359375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zHYB7k9q3O8T"
      },
      "source": [
        "### Why are we getting such different results if we re-run the model?\n",
        "\n",
        "<https://machinelearningmastery.com/randomness-in-machine-learning/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ueDVpctAzvy8"
      },
      "source": [
        "# What architecture should we try?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6W2Sc7-LzQo_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "01876cc3-058b-47d5-ae15-f11ed2ae94a0"
      },
      "source": [
        "model_improved = Sequential()\n",
        "\n",
        "# Input + 1 First Hidden\n",
        "model_improved.add(Dense(10, input_dim=8, activation='sigmoid'))\n",
        "# Hidden\n",
        "model_improved.add(Dense(3, activation='sigmoid'))\n",
        "# Output\n",
        "model_improved.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_improved.compile(loss='binary_crossentropy', \n",
        "                       optimizer='adam',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "model_improved.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 10)                90        \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 33        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 127\n",
            "Trainable params: 127\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVeNM9g64NZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "641686eb-420e-40e9-b1da-33f13427c433"
      },
      "source": [
        "model_improved.fit(X,y, epochs=150, batch_size=32, verbose=0)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07805f8a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdCGOJXi4NZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "254a9f23-a2ee-4d40-ca75-4c785049cd4d"
      },
      "source": [
        "score = model_improved.evaluate(X,y)\n",
        "print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 88us/sample - loss: 0.5787 - acc: 0.6888\n",
            "acc: 65.10416865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tcjMuxtn6wIQ"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "What is an activation function and how does it work?\n",
        "\n",
        "- Takes in a weighted sum of inputs + a bias from the previous layer and outputs an \"activation\" value.\n",
        "- Based its inputs the neuron decides how 'activated' it should be. This can be thought of as the neuron deciding how strongly to fire. You can also think of it as if the neuron is deciding how much of the signal that it has received to pass onto the next layer. \n",
        "- Our choice of activation function does not only affect signal that is passed forward but also affects the backpropagation algorithm. It affects how we update weights in reverse order since activated weight/input sums become the inputs of the next layer. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n_b0u8Ch60bA"
      },
      "source": [
        "## Step Function\n",
        "\n",
        "![Heaviside Step Function](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/325px-Dirac_distribution_CDF.svg.png)\n",
        "\n",
        "All or nothing, a little extreme, which is fine, but makes updating weights through backpropagation impossible. Why? remember that during backpropagation we use derivatives in order to determine how much to update or not update weights. What is the derivative of the step function?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO_Fn7cVG--e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step Function\n",
        "# If it is below a threshold don't activate and if it is above a threshold do activate\n",
        "# It's like a switch\n",
        "# This is a very extreme bianary function.\n",
        "# Why is this a problem? "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKR0YhIVEnXZ"
      },
      "source": [
        "## Linear Function\n",
        "\n",
        "![Linear Function](http://www.roconnell.net/Parent%20function/linear.gif)\n",
        "\n",
        "The linear function takes the opposite tact from the step function and passes the signal onto the next layer by a constant factor. There are problems with this but the biggest problems again lie in backpropagation. The derivative of any linear function is a horizontal line which would indicate that we should update all weights by a constant amount every time -which on balance wouldn't change the behavior of our network. Linear functions are typically only used for very simple tasks where interpretability is important, but if interpretability is your highest priority, you probably shouldn't be using neural networks in the first place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrSDycf1M9IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear Function\n",
        "# Passing the weights in as they are and not changing them.\n",
        "# The derivative is a slope = 1, the rate of change is constant\n",
        "# This means that the weights will be updated by a constant amount every time.\n",
        "# Not so logical?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFurIVL6EkQ8"
      },
      "source": [
        "## Sigmoid Function\n",
        "\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)\n",
        "\n",
        "The sigmoid function works great as an activation function! it's continuously differentiable, its derivative doesn't have a constant slope, and having the higher slope in the middle pushes y value predictions towards extremes which is particularly useful for binary classification problems. I mean, this is why we use it as the squishifier in logistic regression as well. It constrains output, but over repeated epochs pushes predictions towards a strong binary prediction. \n",
        "\n",
        "What's the biggest problem with the sigmoid function? The fact that its slope gets pretty flat so quickly after its departure from zero. This means that updating weights based on its gradient really diminishes the size of our weight updates as our model gets more confident about its classifications. This is why even after so many iterations with our test score example we couldn't reach the levels of fit that our gradient descent based model could reach in just a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6potl3DuNlUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid Function\n",
        "# The sigmoid is a cumulative distribution function.\n",
        "# Has a non-constant slope and continuaslly differentiable. Therefore, the weights change incrementally.\n",
        "# What is the problem? The slope gets flat quickly."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hm6p1HWbEhYi"
      },
      "source": [
        "## Tanh Function\n",
        "\n",
        "![Tanh Function](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
        "\n",
        "What if the sigmoid function didn't get so flat quite as soon when moving away from zero and was a little bit steeper in the middle? That's basically the Tanh function. The Tanh function can actually be created by scaling the sigmoid function by 2 in the y dimension and subtracting 1 from all values. It has basically the same properties as the sigmoid, still struggles from diminishingly flat gradients as we move away from 0, but its derivative is higher around 0 causing weights to move to the extremes a little faster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvVgI1Y5ONxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tanh Function\n",
        "# Similar to sigmoid, but takes longer to get flat and is steaper in the middle."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sFOn_L6gEcz1"
      },
      "source": [
        "## ReLU Function\n",
        "\n",
        "![ReLU Function](https://cdn-images-1.medium.com/max/937/1*oePAhrm74RNnNEolprmTaQ.png)\n",
        "\n",
        "ReLU stands for Rectified Linear Units it is by far the most commonly used activation function in modern neural networks. It doesn't activate neurons that are being passed a negative signal and passes on positive signals. Think about why this might be useful. Remember how a lot of our initial weights got set to negative numbers by chance? This would have dealt with those negative weights a lot faster than the sigmoid function updating. What does the derivative of this function look like? It looks like the step function! This means that not all neurons are activated. With sigmoid basically all of our neurons are passing some amount of signal even if it's small making it hard for the network to differentiate important and less important connections. ReLU turns off a portion of our less important neurons which decreases computational load, but also helps the network learn what the most important connections are faster. \n",
        "\n",
        "What's the problem with relu? Well the left half of its derivative function shows that for neurons that are initialized with weights that cause them to have no activation, our gradient will not update those neuron's weights, this can lead to dead neurons that never fire and whose weights never get updated. We would probably want to update the weights of neurons that didn't fire even if it's just by a little bit in case we got unlucky with our initial weights and want to give those neurons a chance of turning back on in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA4oClP_Opo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ReLU Function\n",
        "# On the one hand acts like a bianary function (either we act or we don't)\n",
        "# But when we activate we mean buisiness.\n",
        "# What is not good? You may have dead neurons."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XWdvWOBIETwk"
      },
      "source": [
        "## Leaky ReLU\n",
        "\n",
        "![Leaky ReLU](https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png)\n",
        "\n",
        "Leaky ReLU accomplishes exactly that! it avoids having a gradient of 0 on the left side of its derivative function. This means that even \"dead\" neurons have a chance of being revived over enough iterations. In some specifications the slope of the leaky left-hand side can also be experimented with as a hyperparameter of the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVnxv0c7ezp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Leaky ReLU\n",
        "# Givnes you a chance to revive dead neurons. They still have negative weight."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FcAxkNFREMFb"
      },
      "source": [
        "## Softmax Function\n",
        "\n",
        "![Softmax Function](https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png)\n",
        "\n",
        "Like the sigmoid function but more useful for multi-class classification problems. The softmax function can take any set of inputs and translate them into probabilities that sum up to 1. This means that we can throw any list of outputs at it and it will translate them into probabilities, this is extremely useful for multi-class classification problems. Like MNIST for example..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC3Ji5SFfJIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Softmac Function\n",
        "# Sigmoid like for multi class classification.\n",
        "# Good for mnist."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "23-XRRXKHs34"
      },
      "source": [
        "## Major takeaways\n",
        "\n",
        "- ReLU is generally better at obtaining the optimal model fit.\n",
        "- Sigmoid and its derivatives are usually better at classification problems.\n",
        "- Softmax for multi-class classification problems. \n",
        "\n",
        "You'll typically see ReLU used for all initial layers and then the final layer being sigmoid or softmax for classification problems. But you can experiment and tune these selections as hyperparameters as well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TWuoXZCCKCI7"
      },
      "source": [
        "## MNIST with Keras \n",
        "\n",
        "### This will be a good chance to bring up dropout regularization. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmJ_5azs04pU",
        "colab": {}
      },
      "source": [
        "### Let's do it!\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PySnnJcH4NaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting hype-parameters\n",
        "\n",
        "batch_size = 64 # how many examples per-epoch\n",
        "num_class = 10 # number of output classes\n",
        "epochs = 150 \n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah-rqhUWgWS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e29dd96-5670-46f3-9135-2b799cc3eeb5"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrEEC1wt4NaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c533dce9-3473-408e-f8be-f48bd22a1365"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFt324YWgiea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32d42f53-cc7e-4eff-c261-2aefa33bc74f"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NuWsJ-N4Naf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "# Flatten the image data\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j193T3Xog0oW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef1daf9a-da31-4ec5-faee-8f6103e84187"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qxjD8dr4Nal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cast them to float from int\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22A9phKh4Nap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converst a class vector if int to bianary class matrix (0s and 1s)\n",
        "y_train = keras.utils.to_categorical(y_train, num_class)\n",
        "y_test = keras.utils.to_categorical(y_test, num_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHJtqfcP4Nat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "902201c3-90f6-44ec-ad89-4d509c1e1513"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZOrIop6hgTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c240056c-32eb-4022-bdc9-dd6325079d46"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
              "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
              "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
              "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
              "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
              "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
              "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
              "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
              "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
              "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
              "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
              "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
              "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfZZ6BqMhno_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ba2118f-9ef8-41ce-b297-71257c78417e"
      },
      "source": [
        "# Scale the X data to also be 0-1. Tends to have better properties\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "X_train[0]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
              "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
              "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
              "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
              "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
              "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
              "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
              "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
              "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
              "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
              "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
              "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
              "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
              "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
              "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
              "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
              "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
              "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
              "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOH09Hz04Nay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "92f32c76-d009-4284-ca20-76f4cf586cbe"
      },
      "source": [
        "mnist_model = Sequential()\n",
        "\n",
        "# Hidden Layer\n",
        "mnist_model.add(Dense(16, input_shape=(784,), activation='relu'))\n",
        "mnist_model.add(Dense(16, activation='relu'))\n",
        "# Output Layer\n",
        "mnist_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "mnist_model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='adam', \n",
        "                    metrics=['accuracy'])\n",
        "mnist_model.summary()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 16)                12560     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 13,002\n",
            "Trainable params: 13,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT7oOlFX4Na6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "2f6528c4-083e-4d8e-a3dd-13194ca9fa85"
      },
      "source": [
        "history = mnist_model.fit(X_train, y_train, epochs=20, validation_split=.1)\n",
        "scores = mnist_model.evaluate(X_test, y_test)\n",
        "print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.1029 - acc: 0.9679 - val_loss: 0.1343 - val_acc: 0.9617\n",
            "Epoch 2/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.1000 - acc: 0.9693 - val_loss: 0.1354 - val_acc: 0.9625\n",
            "Epoch 3/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0981 - acc: 0.9698 - val_loss: 0.1335 - val_acc: 0.9633\n",
            "Epoch 4/20\n",
            "54000/54000 [==============================] - 3s 62us/sample - loss: 0.0942 - acc: 0.9711 - val_loss: 0.1349 - val_acc: 0.9647\n",
            "Epoch 5/20\n",
            "54000/54000 [==============================] - 3s 61us/sample - loss: 0.0933 - acc: 0.9719 - val_loss: 0.1398 - val_acc: 0.9610\n",
            "Epoch 6/20\n",
            "54000/54000 [==============================] - 3s 60us/sample - loss: 0.0916 - acc: 0.9719 - val_loss: 0.1363 - val_acc: 0.9625\n",
            "Epoch 7/20\n",
            "54000/54000 [==============================] - 4s 69us/sample - loss: 0.0884 - acc: 0.9731 - val_loss: 0.1394 - val_acc: 0.9647\n",
            "Epoch 8/20\n",
            "54000/54000 [==============================] - 3s 64us/sample - loss: 0.0875 - acc: 0.9730 - val_loss: 0.1348 - val_acc: 0.9650\n",
            "Epoch 9/20\n",
            "54000/54000 [==============================] - 3s 61us/sample - loss: 0.0852 - acc: 0.9735 - val_loss: 0.1402 - val_acc: 0.9640\n",
            "Epoch 10/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0832 - acc: 0.9743 - val_loss: 0.1468 - val_acc: 0.9598\n",
            "Epoch 11/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0827 - acc: 0.9748 - val_loss: 0.1363 - val_acc: 0.9633\n",
            "Epoch 12/20\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.0809 - acc: 0.9748 - val_loss: 0.1454 - val_acc: 0.9603\n",
            "Epoch 13/20\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.0789 - acc: 0.9759 - val_loss: 0.1468 - val_acc: 0.9618\n",
            "Epoch 14/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0779 - acc: 0.9757 - val_loss: 0.1603 - val_acc: 0.9577\n",
            "Epoch 15/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0768 - acc: 0.9760 - val_loss: 0.1442 - val_acc: 0.9638\n",
            "Epoch 16/20\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.0756 - acc: 0.9762 - val_loss: 0.1441 - val_acc: 0.9628\n",
            "Epoch 17/20\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.0744 - acc: 0.9766 - val_loss: 0.1491 - val_acc: 0.9603\n",
            "Epoch 18/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0729 - acc: 0.9779 - val_loss: 0.1582 - val_acc: 0.9597\n",
            "Epoch 19/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0726 - acc: 0.9778 - val_loss: 0.1505 - val_acc: 0.9622\n",
            "Epoch 20/20\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0710 - acc: 0.9776 - val_loss: 0.1531 - val_acc: 0.9610\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 0.1764 - acc: 0.9539\n",
            "acc: 95.38999795913696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NK3IJUwkhej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f06a6757-1e82-45ab-b388-59ee3ac2d5d0"
      },
      "source": [
        "scores"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1763659995937487, 0.9539]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWDopERJ16yJ",
        "colab": {}
      },
      "source": [
        "### Try it with dropout\n",
        "\n",
        "def prepare_data(X):\n",
        "    \n",
        "    # Reshape x\n",
        "    # make sure everything a float\n",
        "    # etc.\n",
        "    return X_prepared\n",
        "\n",
        "y_pred = mnist_model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CKmx8153w9Ci"
      },
      "source": [
        "## What if we use dropout techniques to prevent overfitting? How does that affect our model?\n",
        "\n",
        "![Regularization](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/354px-Regularization.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYn7caik4NbE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e856d279-7a9c-422e-c2af-19788cc78436"
      },
      "source": [
        "### Let's do it!\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "mnist_model = Sequential()\n",
        "\n",
        "# Hidden\n",
        "mnist_model.add(Dense(32, input_dim=784, activation='relu'))\n",
        "mnist_model.add(Dropout(0.2))\n",
        "mnist_model.add(Dense(16, activation='relu'))\n",
        "mnist_model.add(Dropout(0.2))\n",
        "# Output Layer\n",
        "mnist_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "mnist_model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='adam', \n",
        "                    metrics=['accuracy'])\n",
        "mnist_model.summary()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_39 (Dense)             (None, 32)                25120     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 25,818\n",
            "Trainable params: 25,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuBZaI1j4NbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9feeb676-4a1c-4c01-d1a1-3c4ba2f8d45d"
      },
      "source": [
        "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=20, validation_split=.1, verbose=0)\n",
        "scores = mnist_model.evaluate(X_test, y_test)\n",
        "print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 32us/sample - loss: 0.1588 - acc: 0.9588\n",
            "acc: 95.88000178337097\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}